ðŸš€â€¯LeRobot Setup on Lambdaâ€¯GH200Â (CUDAâ€¯12.8,â€¯ARM64)
This guide walks through setting up the LeRobot repository on a LambdaÂ Cloud GH200 machine (ARM64, CUDAÂ 12.8).

Component	Spec
GPU	NVIDIA GH200â€¯(Hopper,â€¯SM120)
CPU	ARM64
RAM	â€¯â‰ˆâ€¯96â€¯GB
OS	Â Linux (ARM64)

ðŸ“¦â€¯Installation
1â€¯â€¯Installâ€¯MiniforgeÂ (ARM64)
bash
Copy
Edit
cd ~
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh
bash Miniforge3-Linux-aarch64.sh
source ~/.bashrc   # or ~/.zshrc if you use Zsh
conda --version    # sanityâ€‘check
###Â 2â€¯â€¯Cloneâ€¯LeRobot

bash
Copy
Edit
git clone https://github.com/lerobot/lerobot.git
cd lerobot
###Â 3â€¯â€¯Create &â€¯Activate Environment

bash
Copy
Edit
conda create -y -n lerobot python=3.10
conda activate lerobot
###â€¯4â€¯â€¯Install CoreÂ Deps

bash
Copy
Edit
conda install -y ffmpeg -c conda-forge
pip install -e .
###â€¯5â€¯â€¯Remove Any ExistingÂ Torch

bash
Copy
Edit
pip uninstall -y torch torchvision torchaudio torchcodec 2>/dev/null || true
conda remove   -y pytorch torchvision torchaudio pytorch-cuda 2>/dev/null || true
###â€¯6â€¯â€¯Install PyTorchÂ Nightly (CUDAÂ 12.8)

bash
Copy
Edit
pip install --pre --upgrade \
  --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
  torch torchvision
âœ…â€¯SanityÂ Check
bash
Copy
Edit
python - <<'PY'
import torch, os, sys
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda)
x = torch.randn(4096, 4096, device='cuda')
torch.cuda.synchronize()
print("Max abs:", (x @ x.T).abs().max())
PY
You should see the correct Torch nightly version (â‰¥â€¯2.9.0.dev*) and CUDA:Â 12.8.

ðŸâ€¯Training Configuration
Typical train.py snippet:

yaml
Copy
Edit
num_workers: 16
batch_size: 48
steps: 100_000
eval_freq: 20_000
log_freq: 200
These defaults assume plenty of RAM/VRAM; feel free to tune.

ðŸš€â€¯Launch Training
bash
Copy
Edit
python -m ts.train --config-path=configs/train/<your_config>.yaml
Replace <your_config>.yaml with the actual config file for your task.

ðŸ§ â€¯ProÂ Tips
Use watch -n1 nvidia-smi to monitor GPU memory and temps.

Save checkpoints every 10â€‘20â€¯k steps to guard against interruptions.

WANDB logging at log_freq=200 is a good balance between detail and overhead.

ðŸ”—â€¯Resources
LeRobot repo: https://github.com/lerobot/lerobot

PyTorch nightly wheels: https://download.pytorch.org/whl/nightly/

Lambda Cloud docs: https://lambdalabs.com/service/cloud-gpu
